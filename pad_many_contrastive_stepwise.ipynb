{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import asdict\n",
    "from functools import partial\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from config import TrainArgs\n",
    "from data import (\n",
    "    load_kasa_regression,\n",
    "    multiple_contrastive_collate,\n",
    "    tokenize,\n",
    ")\n",
    "from model import MultiContrastiveModel\n",
    "from utils import set_seeds\n",
    "\n",
    "args = TrainArgs(num_dataloader_workers=0)\n",
    "wandb.init(project=\"delta\", name=\"init_v10\", config=asdict(args))\n",
    "\n",
    "set_seeds(args.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_kasa_regression(args)\n",
    "n_train = len(ds[\"train\"])\n",
    "\n",
    "rand_idxs = [\n",
    "    random.randint(0, n_train - 1)\n",
    "    for _ in range(int(args.train_undersample_ratio * n_train))\n",
    "]\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].select(rand_idxs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "tok_func = partial(tokenize, tokenizer=tokenizer)\n",
    "ds = ds.map(tok_func, num_proc=8).remove_columns([\"smiles\", \"inchi\"])\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "padding_collator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import MultiContrastiveTrainer, initialize_train_dataloader\n",
    "\n",
    "model = MultiContrastiveModel(\n",
    "    AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    ").cuda()\n",
    "\n",
    "train_dl = initialize_train_dataloader(ds['train'], model, padding_collator, args)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=args.lr)\n",
    "n_epochs = args.n_epochs\n",
    "n_steps = len(train_dl) * n_epochs\n",
    "warmup_ratio = args.warmup_ratio\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, n_steps * warmup_ratio, n_steps * (1 - warmup_ratio)\n",
    ")\n",
    "\n",
    "trainer = MultiContrastiveTrainer(model, optimizer, scheduler, train_dl, ds['test'], args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
